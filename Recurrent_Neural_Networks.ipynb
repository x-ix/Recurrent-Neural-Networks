{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Language Models and the Dataset\n",
    "\n",
    "* In the previous lecture, saw above how to map text data into tokens, where these tokens can be words or characters.\n",
    "* Assume that a text sequence of $T$ tokens: $x_1, x_2, \\ldots, x_T$. \n",
    "* Given such a sequence, the goal of a **language model** is to estimate the joint probability of the sequence\n",
    "\n",
    "$$P(x_1, x_2, \\ldots, x_T).$$\n",
    "\n",
    "## Applications of Language Models\n",
    "\n",
    "* Could generate natural text just on its own, simply by drawing one token at a time $x_t \\sim P(x_t \\mid x_{t-1}, \\ldots, x_1)$.\n",
    "* Could generate a meaningful dialog, simply by conditioning the text on previous dialog fragments.\n",
    "* Help improve Speech Recognition \n",
    "    * when two words or phrases sound similar (e.g. phrases \"to recognize speech\" and \"to wreck a nice beach\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Learning a Language Model\n",
    "\n",
    "* Suppose that we tokenize text data at the word level.\n",
    "* By applying basic probability rules:\n",
    "\n",
    "$$P(x_1, x_2, \\ldots, x_T) = P(x_1)P(x_2 \\mid x_1)P(x_3 \\mid x_1,x_2)P(x_4 \\mid x_1,x_2,x_3)$$\n",
    "\n",
    "* Example:\n",
    "\n",
    "$$P(\\text{deep}, \\text{learning}, \\text{is}, \\text{fun}) =  P(\\text{deep}) P(\\text{learning}  \\mid  \\text{deep}) P(\\text{is}  \\mid  \\text{deep}, \\text{learning}) P(\\text{fun}  \\mid  \\text{deep}, \\text{learning}, \\text{is}).$$\n",
    "\n",
    "* So in order to compute the language model, we need to calculate the probability of words and the conditional probability of a word given the previous few words.\n",
    "* Such probabilities are essentially the language model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The probability of words can be calculated from the relative word frequency of a given word in the training dataset.\n",
    "    * For example, the estimate $\\hat{P}(\\text{deep})$ can be calculated by counting all occurrences of the word \"deep\" and dividing it by the total number of words in the corpus.\n",
    "* Moving on, we wish to estimate\n",
    "$$\\hat{P}(\\text{learning} \\mid \\text{deep}) = \\frac{n(\\text{deep, learning})}{n(\\text{deep})},$$\n",
    "where $n(x)$ and $n(x, x')$ are the number of occurrences of single\n",
    "and consecutive word pairs, respectively.\n",
    "\n",
    "* Estimating the probability of a word pair is somewhat more difficult, since the occurrences of \"deep learning\" are a lot less frequent. \n",
    "* The number of model parameters increases exponentially with the number of tokens we condition upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iI2MCTgQ6hJi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "\n",
    "* Rather than modeling $P(x_t \\mid x_{t-1}, \\ldots, x_{t-n+1})$ it is preferable to use a latent variable model:\n",
    "$$P(x_t \\mid x_{t-1}, \\ldots, x_1) \\approx P(x_t \\mid h_{t-1}),$$\n",
    "where $h_{t-1}$ is a *hidden state* (also known as a hidden variable) that stores the sequence information up to time step $t-1$.\n",
    "\n",
    "* The hidden state at any time step $t$ can be computed based on both the current input $x_{t}$ and the previous hidden state $h_{t-1}$:\n",
    "\n",
    "$$h_t = f(x_{t}, h_{t-1}).$$\n",
    "\n",
    "* **Recurrent neural networks (RNNs)** are neural networks with *hidden states*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaOu9t4g6hJz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks without Hidden States\n",
    "\n",
    "* Before introducing the RNN model, we first revisit the MLP model.\n",
    "    * We take a look at an MLP with a single hidden layer.\n",
    "    * Let the hidden layer's activation function be $\\phi$.\n",
    "    \n",
    "* Given a minibatch of examples $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ with batch size $n$ and $d$ inputs, the hidden layer's output $\\mathbf{H} \\in \\mathbb{R}^{n \\times h}$ is calculated as\n",
    "$$\\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W}_{xh} + \\mathbf{b}_h),$$\n",
    "where, for the hidden layer, we denote the weights by $\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}$, the bias by $\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$, and the number of hidden units by $h$. \n",
    "\n",
    "* Next, the hidden variable $\\mathbf{H}$ is used as the input of the output layer. The output layer is given by\n",
    "$$\\mathbf{O} = \\mathbf{H} \\mathbf{W}_{hq} + \\mathbf{b}_q,$$\n",
    "where $\\mathbf{O} \\in \\mathbb{R}^{n \\times q}$ is the output variable, $\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$ are the weights, and $\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$ is the bias of the output layer.  \n",
    "    * If it is a classification problem, we can use $\\text{softmax}(\\mathbf{O})$ to compute the probability distribution of the output categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvsPmSsS6hJ0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Recurrent Neural Networks with Hidden States\n",
    "\n",
    "* Assume that we have a minibatch of inputs $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$ at time step $t$.\n",
    "    * In other words, for a minibatch of $n$ sequence examples, each row of $\\mathbf{X}_t$ corresponds to one example at time step $t$ from the sequence.\n",
    "* Denote by $\\mathbf{H}_t  \\in \\mathbb{R}^{n \\times h}$ the hidden variable of time step $t$.\n",
    "* Unlike the MLP, here we save the hidden variable $\\mathbf{H}_{t-1}$ from the previous time step and introduce a new weight parameter $\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$ to describe how to use the hidden variable of the previous time step in the current time step. \n",
    "\n",
    "* Specifically, the calculation of the hidden variable of the current time step is determined by the input of the current time step together with the hidden variable of the previous time step:\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h).$$\n",
    "\n",
    "* Compared to MLP adds one more term $\\mathbf{H}_{t-1} \\mathbf{W}_{hh}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05kl0gdF6hJ1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The hidden variable $\\mathbf{H}_{t}$ captures and retains the sequence's historical information up to their current time step. \n",
    "    * Such a hidden variable is called a *hidden state*.\n",
    "\n",
    "* For time step $t$, the output of the output layer is similar to the computation in the MLP:\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.$$\n",
    "\n",
    "* RNN parameters include:\n",
    "    * the weights $\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$, and the bias $\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$ of the hidden layer,\n",
    "    * the weights $\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$ and the bias $\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$ of the output layer.\n",
    "* Even at different time steps, RNNs always use these model parameters.\n",
    "    * The parameters of an RNN do not grow as the number of time steps increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* At any time step $t$, the computation of the hidden state $\\mathbf{H}_t$ can be done as follows:\n",
    "  1. concatenate the input $\\mathbf{X}_t$ at the current time step $t$ and the hidden state $\\mathbf{H}_{t-1}$ at the previous time step $t-1$;\n",
    "  1. concatenate the weights $\\mathbf{W}_{xh}$ and  $\\mathbf{W}_{hh}$\n",
    "  1. Feeding the concatenated input into a fully-connected layer with the concatenated weights (and the activation function $\\phi$).\n",
    "  1. The output of such a fully-connected layer is the hidden state $\\mathbf{H}_t$ of the current time step $t$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRyuNMtT6hJ3",
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "* The figure below illustrates the computational logic of an RNN at three adjacent time steps.\n",
    "\n",
    "![An RNN with a hidden state.](img/rnn.svg) \n",
    "\n",
    "<!-- ![An RNN with a hidden state.](https://drive.google.com/uc?export=view&id=1w60hcT4EEbsw_I42wUF4TRTRdzFDMZPE) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXwuZOQ36hJ4",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN-based Character-Level Language Models\n",
    "\n",
    "* For simplicity, we tokenize text into characters rather than words. \n",
    "\n",
    "* The following figure demonstrates how an RNN predicts the next character based on the current and previous characters:\n",
    "\n",
    "![A character-level language model based on the RNN. The input and label sequences are \"machin\" and \"achine\", respectively.](img/rnn-train.svg) \n",
    "\n",
    "<!-- ![A character-level language model based on the RNN. The input and label sequences are \"machin\" and \"achine\", respectively.](https://drive.google.com/uc?export=view&id=19GV8yiSAxBvRZxvpfQKkfKWZDDh9oZGG) -->\n",
    "\n",
    "* During the training process, a softmax operation is applied on the output from the output layer for each time step, and cross-entropy loss is used to compute the error between the model output and the label.\n",
    "* Due to the recurrent computation, the output of time step 3  $\\mathbf{O}_3$, is determined by the text sequence \"m\", \"a\", and \"c\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2n6EEZkL6hJ8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "* A neural network that uses recurrent computation for hidden states is called a recurrent neural network (RNN).\n",
    "* The hidden state of an RNN can capture historical information of the sequence up to the current time step.\n",
    "* The number of RNN model parameters does not grow as the number of time steps increases.\n",
    "* We can create character-level language models using an  RNN."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "Recurrent_Neural_Networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
